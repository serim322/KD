Found cached dataset maccrobat_biomedical_ner (/home/s1/serimkim/.cache/huggingface/datasets/singh-aditya___maccrobat_biomedical_ner/default/0.0.0/974acf92c41cf1d5ddfe4bc372d577351657cfae1f57fe3899bbfc4c05ed09fc)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 370.55it/s]
Some weights of the model checkpoint at /home/s1/serimkim/hf/distil-kobert-finetuned_len10_epoch5/checkpoint-13000 were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at /home/s1/serimkim/hf/distil-kobert-finetuned_len10_epoch5/checkpoint-13000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /home/s1/serimkim/.cache/huggingface/datasets/singh-aditya___maccrobat_biomedical_ner/default/0.0.0/974acf92c41cf1d5ddfe4bc372d577351657cfae1f57fe3899bbfc4c05ed09fc/cache-a8a8aa885433f07f.arrow
Map:   0%|          | 0/160 [00:00<?, ? examples/s]Map: 100%|██████████| 160/160 [00:00<00:00, 382.76 examples/s]                                                              Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Using amp fp16 backend
***** Running training *****
  Num examples = 1215
  Num Epochs = 2
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 20
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ksr5970. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/s1/serimkim/hf/wandb/run-20240122_050045-zvcfofqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuned_distilkobert_ner_10
wandb: ⭐️ View project at https://wandb.ai/ksr5970/huggingface
wandb: 🚀 View run at https://wandb.ai/ksr5970/huggingface/runs/zvcfofqf
cuda
  0%|          | 0/20 [00:00<?, ?it/s]/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  5%|▌         | 1/20 [00:06<02:07,  6.69s/it] 10%|█         | 2/20 [00:06<00:52,  2.89s/it] 15%|█▌        | 3/20 [00:07<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:17,  1.10s/it] 25%|██▌       | 5/20 [00:07<00:11,  1.28it/s] 30%|███       | 6/20 [00:07<00:08,  1.69it/s] 35%|███▌      | 7/20 [00:08<00:06,  2.13it/s] 40%|████      | 8/20 [00:08<00:04,  2.56it/s] 45%|████▌     | 9/20 [00:08<00:03,  2.96it/s] 50%|█████     | 10/20 [00:08<00:02,  3.42it/s]***** Running Evaluation *****
  Num examples = 321
  Batch size = 128

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 14.94it/s][A/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                               
                                             [A 50%|█████     | 10/20 [00:09<00:02,  3.42it/s]
100%|██████████| 3/3 [00:00<00:00, 14.94it/s][A
                                             [ASaving model checkpoint to finetuned_distilkobert_ner_10/checkpoint-10
Configuration saved in finetuned_distilkobert_ner_10/checkpoint-10/config.json
Model weights saved in finetuned_distilkobert_ner_10/checkpoint-10/pytorch_model.bin
tokenizer config file saved in finetuned_distilkobert_ner_10/checkpoint-10/tokenizer_config.json
Special tokens file saved in finetuned_distilkobert_ner_10/checkpoint-10/special_tokens_map.json
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 55%|█████▌    | 11/20 [00:10<00:06,  1.37it/s] 60%|██████    | 12/20 [00:10<00:04,  1.73it/s] 65%|██████▌   | 13/20 [00:10<00:03,  2.14it/s] 70%|███████   | 14/20 [00:11<00:02,  2.54it/s] 75%|███████▌  | 15/20 [00:11<00:01,  2.92it/s] 80%|████████  | 16/20 [00:11<00:01,  3.27it/s] 85%|████████▌ | 17/20 [00:11<00:00,  3.57it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.81it/s] 95%|█████████▌| 19/20 [00:12<00:00,  3.99it/s]100%|██████████| 20/20 [00:12<00:00,  4.31it/s]***** Running Evaluation *****
  Num examples = 321
  Batch size = 128
{'eval_loss': 3.945547580718994, 'eval_precision': 0.06298904538341157, 'eval_recall': 0.030434782608695653, 'eval_f1': 0.04104002039255671, 'eval_accuracy': 0.3174234424498416, 'eval_runtime': 0.605, 'eval_samples_per_second': 530.608, 'eval_steps_per_second': 4.959, 'epoch': 1.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 14.73it/s][A/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                               
                                             [A100%|██████████| 20/20 [00:12<00:00,  4.31it/s]
100%|██████████| 3/3 [00:00<00:00, 14.73it/s][A
                                             [ASaving model checkpoint to finetuned_distilkobert_ner_10/checkpoint-20
Configuration saved in finetuned_distilkobert_ner_10/checkpoint-20/config.json
Model weights saved in finetuned_distilkobert_ner_10/checkpoint-20/pytorch_model.bin
tokenizer config file saved in finetuned_distilkobert_ner_10/checkpoint-20/tokenizer_config.json
Special tokens file saved in finetuned_distilkobert_ner_10/checkpoint-20/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from finetuned_distilkobert_ner_10/checkpoint-20 (score: 3.7659809589385986).
                                               100%|██████████| 20/20 [00:13<00:00,  4.31it/s]100%|██████████| 20/20 [00:13<00:00,  1.43it/s]
{'eval_loss': 3.7659809589385986, 'eval_precision': 0.05774278215223097, 'eval_recall': 0.00831758034026465, 'eval_f1': 0.01454064771976206, 'eval_accuracy': 0.41003167898627246, 'eval_runtime': 0.5856, 'eval_samples_per_second': 548.128, 'eval_steps_per_second': 5.123, 'epoch': 2.0}
{'train_runtime': 20.732, 'train_samples_per_second': 117.21, 'train_steps_per_second': 0.965, 'train_loss': 4.029591369628906, 'epoch': 2.0}
[{'eval_loss': 3.945547580718994, 'eval_precision': 0.06298904538341157, 'eval_recall': 0.030434782608695653, 'eval_f1': 0.04104002039255671, 'eval_accuracy': 0.3174234424498416, 'eval_runtime': 0.605, 'eval_samples_per_second': 530.608, 'eval_steps_per_second': 4.959, 'epoch': 1.0, 'step': 10}, {'eval_loss': 3.7659809589385986, 'eval_precision': 0.05774278215223097, 'eval_recall': 0.00831758034026465, 'eval_f1': 0.01454064771976206, 'eval_accuracy': 0.41003167898627246, 'eval_runtime': 0.5856, 'eval_samples_per_second': 548.128, 'eval_steps_per_second': 5.123, 'epoch': 2.0, 'step': 20}, {'train_runtime': 20.732, 'train_samples_per_second': 117.21, 'train_steps_per_second': 0.965, 'total_flos': 104015748602880.0, 'train_loss': 4.029591369628906, 'epoch': 2.0, 'step': 20}]
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁█
wandb:                        eval/f1 █▁
wandb:                      eval/loss █▁
wandb:                 eval/precision █▁
wandb:                    eval/recall █▁
wandb:                   eval/runtime █▁
wandb:        eval/samples_per_second ▁█
wandb:          eval/steps_per_second ▁█
wandb:                    train/epoch ▁██
wandb:              train/global_step ▁██
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.41003
wandb:                        eval/f1 0.01454
wandb:                      eval/loss 3.76598
wandb:                 eval/precision 0.05774
wandb:                    eval/recall 0.00832
wandb:                   eval/runtime 0.5856
wandb:        eval/samples_per_second 548.128
wandb:          eval/steps_per_second 5.123
wandb:                    train/epoch 2.0
wandb:              train/global_step 20
wandb:               train/total_flos 104015748602880.0
wandb:               train/train_loss 4.02959
wandb:            train/train_runtime 20.732
wandb: train/train_samples_per_second 117.21
wandb:   train/train_steps_per_second 0.965
wandb: 
wandb: 🚀 View run finetuned_distilkobert_ner_10 at: https://wandb.ai/ksr5970/huggingface/runs/zvcfofqf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240122_050045-zvcfofqf/logs
