Found cached dataset maccrobat_biomedical_ner (/home/s1/serimkim/.cache/huggingface/datasets/singh-aditya___maccrobat_biomedical_ner/default/0.0.0/974acf92c41cf1d5ddfe4bc372d577351657cfae1f57fe3899bbfc4c05ed09fc)
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 605.59it/s]
Some weights of the model checkpoint at /home/s1/serimkim/hf/distil-kobert-finetuned_len10_epoch4/checkpoint-13000 were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at /home/s1/serimkim/hf/distil-kobert-finetuned_len10_epoch4/checkpoint-13000 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /home/s1/serimkim/.cache/huggingface/datasets/singh-aditya___maccrobat_biomedical_ner/default/0.0.0/974acf92c41cf1d5ddfe4bc372d577351657cfae1f57fe3899bbfc4c05ed09fc/cache-a8a8aa885433f07f.arrow
Map:   0%|          | 0/160 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:00<00:00, 399.01 examples/s]                                                              Map:   0%|          | 0/40 [00:00<?, ? examples/s]                                                  Using amp fp16 backend
***** Running training *****
  Num examples = 1237
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 30
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ksr5970. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/s1/serimkim/hf/wandb/run-20240122_050622-zc7sk46n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuned_distilkobert_ner_10
wandb: â­ï¸ View project at https://wandb.ai/ksr5970/huggingface
wandb: ðŸš€ View run at https://wandb.ai/ksr5970/huggingface/runs/zc7sk46n
cuda
  0%|          | 0/30 [00:00<?, ?it/s]/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  3%|â–Ž         | 1/30 [00:05<02:48,  5.80s/it]  7%|â–‹         | 2/30 [00:06<01:10,  2.52s/it] 10%|â–ˆ         | 3/30 [00:06<00:39,  1.46s/it] 13%|â–ˆâ–Ž        | 4/30 [00:06<00:25,  1.03it/s] 17%|â–ˆâ–‹        | 5/30 [00:06<00:17,  1.44it/s] 20%|â–ˆâ–ˆ        | 6/30 [00:06<00:12,  1.89it/s] 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:07<00:09,  2.35it/s] 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:07<00:07,  2.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:07<00:06,  3.21it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:07<00:05,  3.66it/s]***** Running Evaluation *****
  Num examples = 298
  Batch size = 128

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 15.25it/s][A/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                               
                                             [A 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:08<00:05,  3.66it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 15.25it/s][A
                                             [ASaving model checkpoint to finetuned_distilkobert_ner_10/checkpoint-10
Configuration saved in finetuned_distilkobert_ner_10/checkpoint-10/config.json
Model weights saved in finetuned_distilkobert_ner_10/checkpoint-10/pytorch_model.bin
tokenizer config file saved in finetuned_distilkobert_ner_10/checkpoint-10/tokenizer_config.json
Special tokens file saved in finetuned_distilkobert_ner_10/checkpoint-10/special_tokens_map.json
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:09<00:13,  1.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:09<00:10,  1.77it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:09<00:07,  2.19it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:10<00:06,  2.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:10<00:04,  3.02it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:10<00:04,  3.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:10<00:03,  3.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:10<00:03,  3.97it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:11<00:02,  4.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:11<00:02,  4.45it/s]***** Running Evaluation *****
  Num examples = 298
  Batch size = 128
{'eval_loss': 4.007021903991699, 'eval_precision': 0.0019459459459459458, 'eval_recall': 0.0019288469781397343, 'eval_f1': 0.0019373587342589602, 'eval_accuracy': 0.2519412256600167, 'eval_runtime': 0.5654, 'eval_samples_per_second': 527.08, 'eval_steps_per_second': 5.306, 'epoch': 1.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 15.23it/s][A/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                               
                                             [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:11<00:02,  4.45it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 15.23it/s][A
                                             [ASaving model checkpoint to finetuned_distilkobert_ner_10/checkpoint-20
Configuration saved in finetuned_distilkobert_ner_10/checkpoint-20/config.json
Model weights saved in finetuned_distilkobert_ner_10/checkpoint-20/pytorch_model.bin
tokenizer config file saved in finetuned_distilkobert_ner_10/checkpoint-20/tokenizer_config.json
Special tokens file saved in finetuned_distilkobert_ner_10/checkpoint-20/special_tokens_map.json
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:12<00:05,  1.54it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:13<00:04,  1.94it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:13<00:02,  2.35it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:13<00:02,  2.77it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:13<00:01,  3.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:13<00:01,  3.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:14<00:00,  3.81it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:14<00:00,  4.05it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:14<00:00,  4.24it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:14<00:00,  4.51it/s]***** Running Evaluation *****
  Num examples = 298
  Batch size = 128
{'eval_loss': 3.738083839416504, 'eval_precision': 0.0028735632183908046, 'eval_recall': 0.0002143163309044149, 'eval_f1': 0.00039888312724371757, 'eval_accuracy': 0.42778640544737784, 'eval_runtime': 0.5354, 'eval_samples_per_second': 556.566, 'eval_steps_per_second': 5.603, 'epoch': 2.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 15.29it/s][A/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/s1/serimkim/anaconda3/envs/hf/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                               
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:15<00:00,  4.51it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 15.29it/s][A
                                             [ASaving model checkpoint to finetuned_distilkobert_ner_10/checkpoint-30
Configuration saved in finetuned_distilkobert_ner_10/checkpoint-30/config.json
Model weights saved in finetuned_distilkobert_ner_10/checkpoint-30/pytorch_model.bin
tokenizer config file saved in finetuned_distilkobert_ner_10/checkpoint-30/tokenizer_config.json
Special tokens file saved in finetuned_distilkobert_ner_10/checkpoint-30/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from finetuned_distilkobert_ner_10/checkpoint-30 (score: 3.61104679107666).
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:16<00:00,  4.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:16<00:00,  1.83it/s]
{'eval_loss': 3.61104679107666, 'eval_precision': 0.008064516129032258, 'eval_recall': 0.0002143163309044149, 'eval_f1': 0.0004175365344467641, 'eval_accuracy': 0.43650698841237606, 'eval_runtime': 0.5347, 'eval_samples_per_second': 557.353, 'eval_steps_per_second': 5.611, 'epoch': 3.0}
{'train_runtime': 22.7923, 'train_samples_per_second': 162.818, 'train_steps_per_second': 1.316, 'train_loss': 3.9737887064615887, 'epoch': 3.0}
[{'eval_loss': 4.007021903991699, 'eval_precision': 0.0019459459459459458, 'eval_recall': 0.0019288469781397343, 'eval_f1': 0.0019373587342589602, 'eval_accuracy': 0.2519412256600167, 'eval_runtime': 0.5654, 'eval_samples_per_second': 527.08, 'eval_steps_per_second': 5.306, 'epoch': 1.0, 'step': 10}, {'eval_loss': 3.738083839416504, 'eval_precision': 0.0028735632183908046, 'eval_recall': 0.0002143163309044149, 'eval_f1': 0.00039888312724371757, 'eval_accuracy': 0.42778640544737784, 'eval_runtime': 0.5354, 'eval_samples_per_second': 556.566, 'eval_steps_per_second': 5.603, 'epoch': 2.0, 'step': 20}, {'eval_loss': 3.61104679107666, 'eval_precision': 0.008064516129032258, 'eval_recall': 0.0002143163309044149, 'eval_f1': 0.0004175365344467641, 'eval_accuracy': 0.43650698841237606, 'eval_runtime': 0.5347, 'eval_samples_per_second': 557.353, 'eval_steps_per_second': 5.611, 'epoch': 3.0, 'step': 30}, {'train_runtime': 22.7923, 'train_samples_per_second': 162.818, 'train_steps_per_second': 1.316, 'total_flos': 158848742002176.0, 'train_loss': 3.9737887064615887, 'epoch': 3.0, 'step': 30}]
wandb: - 0.014 MB of 0.014 MB uploadedwandb: \ 0.014 MB of 0.014 MB uploadedwandb: | 0.014 MB of 0.014 MB uploadedwandb: / 0.014 MB of 0.014 MB uploadedwandb: - 0.014 MB of 0.014 MB uploadedwandb: \ 0.014 MB of 0.014 MB uploadedwandb: 
wandb: Run history:
wandb:                  eval/accuracy â–â–ˆâ–ˆ
wandb:                        eval/f1 â–ˆâ–â–
wandb:                      eval/loss â–ˆâ–ƒâ–
wandb:                 eval/precision â–â–‚â–ˆ
wandb:                    eval/recall â–ˆâ–â–
wandb:                   eval/runtime â–ˆâ–â–
wandb:        eval/samples_per_second â–â–ˆâ–ˆ
wandb:          eval/steps_per_second â–â–ˆâ–ˆ
wandb:                    train/epoch â–â–…â–ˆâ–ˆ
wandb:              train/global_step â–â–…â–ˆâ–ˆ
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.43651
wandb:                        eval/f1 0.00042
wandb:                      eval/loss 3.61105
wandb:                 eval/precision 0.00806
wandb:                    eval/recall 0.00021
wandb:                   eval/runtime 0.5347
wandb:        eval/samples_per_second 557.353
wandb:          eval/steps_per_second 5.611
wandb:                    train/epoch 3.0
wandb:              train/global_step 30
wandb:               train/total_flos 158848742002176.0
wandb:               train/train_loss 3.97379
wandb:            train/train_runtime 22.7923
wandb: train/train_samples_per_second 162.818
wandb:   train/train_steps_per_second 1.316
wandb: 
wandb: ðŸš€ View run finetuned_distilkobert_ner_10 at: https://wandb.ai/ksr5970/huggingface/runs/zc7sk46n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240122_050622-zc7sk46n/logs
